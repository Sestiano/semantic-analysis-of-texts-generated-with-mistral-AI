{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d36bd7b",
   "metadata": {},
   "source": [
    "# Semantic Analysis Pipeline for LLM-Generated Texts\n",
    "\n",
    "Complete pipeline for analyzing semantic structure of texts generated by local LLMs via LM Studio. Includes text generation, preprocessing, network construction, analysis, and visualization.\n",
    "\n",
    "**Key Features:** BERTScore semantic similarity, network analysis, bootstrap statistics, topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36166088",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoatlas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllm_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMGenerator\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtext_preprocessor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextPreprocessor\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetwork_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NetworkBuilder\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetwork_analyzer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NetworkAnalyzer\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstatistical_tests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StatisticalAnalyzer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/semantic-analysis-of-texts-generated-with-mistral-AI/network_builder.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mglob\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01memoatlas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EmoScores\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Optional\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'emoatlas'"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "from llm_generator import LLMGenerator\n",
    "from text_preprocessor import TextPreprocessor\n",
    "from network_builder import NetworkBuilder\n",
    "from network_analyzer import NetworkAnalyzer\n",
    "from statistical_tests import StatisticalAnalyzer\n",
    "from visualizer import NetworkVisualizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f636b2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Load centralized configuration from `config.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb2e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced configuration completed successfully!\n",
      "Temperature values: [0.001, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5]\n",
      "Prompt types: ['complex', 'vague']\n",
      "Directory structure: {'texts': 'texts', 'results': 'results', 'figures': 'figures', 'visualizations': 'visualizations', 'bootstrap': 'bootstrap_results', 'semantic': 'semantic_analysis'}\n",
      "Enhanced analysis parameters:\n",
      "  - Text completions: 100\n",
      "  - Bootstrap samples: 1000\n",
      "  - Semantic models: ['bertscore', 'fasttext', 'sentence_transformers']\n",
      "  - BERTScore model: distilbert-base-uncased\n",
      "  - Similarity thresholds: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
      "  - Primary threshold: 0.3\n",
      "  - Temperature correlation methods: ['pearson', 'spearman', 'kendall']\n",
      "  - Community detection: ['louvain', 'leiden']\n",
      "  - Effect size measures: ['cohens_d', 'hedges_g', 'eta_squared']\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from config.json\n",
    "with open('config.json', 'r') as f:\n",
    "    CONFIG = json.load(f)\n",
    "\n",
    "# Extract key settings\n",
    "TEMPERATURES = CONFIG['generation']['temperatures']\n",
    "PROMPTS = CONFIG['prompts']\n",
    "DIRS = CONFIG['directories']\n",
    "DIR_PREFIX = CONFIG['generation']['dir_prefix']\n",
    "\n",
    "# Analysis configuration\n",
    "ANALYSIS_CONFIG = {\n",
    "    'n_completions': CONFIG['generation']['n_completions'],\n",
    "    'n_bootstrap': CONFIG['analysis']['n_bootstrap'],\n",
    "    'bootstrap_ci': CONFIG['analysis']['bootstrap_ci'],\n",
    "    'style': CONFIG['analysis']['style'],\n",
    "    'figsize': tuple(CONFIG['analysis']['figsize'])\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_path in DIRS.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Configuration loaded from config.json\")\n",
    "print(f\"  Temperatures: {len(TEMPERATURES)} levels\")\n",
    "print(f\"  Prompts: {len(PROMPTS)} types\")\n",
    "print(f\"  Completions per condition: {ANALYSIS_CONFIG['n_completions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc937546",
   "metadata": {},
   "source": [
    "## 1. Text Generation\n",
    "\n",
    "Generate texts using LM Studio API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2d0b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text generation...\n",
      "Generating 100 completions per condition...\n",
      "Total conditions: 2 prompts × 7 temperatures = 14\n",
      "Starting generation of 14 text sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing vague T=1.5: 100%|██████████| 14/14 [12:50<00:00, 55.04s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation completed!\n",
      "  - Processed prompts: 2\n",
      "  - Temperatures per prompt: 7\n",
      "  - Completions per temperature: 100\n",
      "  - Total texts generated: 1400\n",
      "Text generation completed!\n",
      "Text generation failed. Check your API key and try again.\n",
      "No existing generated texts found. You may need to run the text generation step.\n",
      "Make sure you have a valid Mistral API key and uncomment the code above.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize generator\n",
    "generator = LLMGenerator(\n",
    "    model=CONFIG['lm_studio']['model_name'],\n",
    "    base_url=CONFIG['lm_studio']['url']\n",
    ")\n",
    "\n",
    "# Generate texts\n",
    "generator.generate_texts(\n",
    "    prompts=PROMPTS,\n",
    "    temperatures=TEMPERATURES,\n",
    "    n_completions=ANALYSIS_CONFIG['n_completions'],\n",
    "    texts_dir=DIRS['texts'],\n",
    "    dir_prefix=DIR_PREFIX\n",
    ")\n",
    "\n",
    "# Check results\n",
    "for prompt_type in PROMPTS.keys():\n",
    "    dir_path = os.path.join(DIRS['texts'], f\"{DIR_PREFIX}_{prompt_type}\")\n",
    "    if os.path.exists(dir_path):\n",
    "        count = len(glob.glob(os.path.join(dir_path, '*.txt')))\n",
    "        print(f\"✓ {prompt_type.capitalize()}: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca041d7",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Clean texts using SpaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdfd2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing text preprocessor...\n",
      "Starting text preprocessing...\n",
      "Processing complex prompts...\n",
      "Starting text preprocessing...\n",
      "Processing complex prompts...\n",
      "7 files processed for complex prompts\n",
      "Processing vague prompts...\n",
      "7 files processed for complex prompts\n",
      "Processing vague prompts...\n",
      "7 files processed for vague prompts\n",
      "Text preprocessing completed! Total files processed: 14\n",
      "  - Complex cleaned files: 7\n",
      "  - Vague cleaned files: 7\n",
      "7 files processed for vague prompts\n",
      "Text preprocessing completed! Total files processed: 14\n",
      "  - Complex cleaned files: 7\n",
      "  - Vague cleaned files: 7\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(lang_model=CONFIG['preprocessing']['lang_model'])\n",
    "\n",
    "# Process texts\n",
    "source_dirs = [os.path.join(DIRS['texts'], f\"{DIR_PREFIX}_{pt}\") for pt in PROMPTS.keys()]\n",
    "for source_dir in source_dirs:\n",
    "    if os.path.exists(source_dir):\n",
    "        prompt_type = 'complex' if 'complex' in source_dir else 'vague'\n",
    "        target_dir = os.path.join(DIRS['texts'], f\"cleaned_{DIR_PREFIX}_{prompt_type}\")\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        \n",
    "        for text_file in glob.glob(os.path.join(source_dir, '*.txt')):\n",
    "            filename = os.path.basename(text_file).replace('.txt', '')\n",
    "            output_file = os.path.join(target_dir, f\"{filename}_cleaned.txt\")\n",
    "            preprocessor.clean_single_file(text_file, output_file)\n",
    "\n",
    "# Check results\n",
    "for prompt_type in PROMPTS.keys():\n",
    "    dir_path = os.path.join(DIRS['texts'], f\"cleaned_{DIR_PREFIX}_{prompt_type}\")\n",
    "    if os.path.exists(dir_path):\n",
    "        count = len(glob.glob(os.path.join(dir_path, '*.txt')))\n",
    "        print(f\"✓ {prompt_type.capitalize()} cleaned: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc94a5c",
   "metadata": {},
   "source": [
    "## 3. Network Construction\n",
    "\n",
    "Build semantic networks using EmoAtlas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771f403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network builder...\n",
      "Building semantic networks from preprocessed texts...\n",
      "Found 14 files to process with EmoAtlas...\n",
      "Building semantic networks from preprocessed texts...\n",
      "Found 14 files to process with EmoAtlas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing vague_1.0.txt: 100%|██████████| 14/14 [1:04:37<00:00, 277.00s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EmoAtlas network creation completed!\n",
      "  - Edge lists saved in: emo_edges_complex/ and emo_edges_vague/\n",
      "Semantic network construction completed!\n",
      "  - Complex networks: 7 files created\n",
      "  - Vague networks: 7 files created\n",
      "\n",
      "Edge list files contain network connections:\n",
      "  - Complex example: emo_edge_list_complex_0.25.txt\n",
      "  - Vague example: emo_edge_list_vague_1.0.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize network builder\n",
    "network_builder = NetworkBuilder()\n",
    "\n",
    "# Build networks\n",
    "source_dirs = [os.path.join(DIRS['texts'], f\"{DIR_PREFIX}_{pt}\") for pt in PROMPTS.keys()]\n",
    "network_builder.build_networks_from_texts(\n",
    "    texts_dir=DIRS['texts'],\n",
    "    source_dirs=source_dirs,\n",
    "    dir_prefix=DIR_PREFIX\n",
    ")\n",
    "\n",
    "# Check results\n",
    "for prompt_type in PROMPTS.keys():\n",
    "    edge_dir = f'emo_edges_{prompt_type}'\n",
    "    if os.path.exists(edge_dir):\n",
    "        count = len(glob.glob(os.path.join(edge_dir, '*.txt')))\n",
    "        print(f\"✓ {prompt_type.capitalize()} networks: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b6943",
   "metadata": {},
   "source": [
    "## 4. Network Analysis\n",
    "\n",
    "Calculate network metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d4d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network analyzer...\n",
      "Network analysis already completed!\n",
      "  - Total networks analyzed: 14\n",
      "  - Prompt types: ['complex' 'vague']\n",
      "  - Temperature values: [0.001, 0.25, 0.5, 0.75, 1.0, 1.25, 1.5]\n",
      "  - Metrics calculated: 11 metrics per network\n",
      "\n",
      "Sample of network metrics:\n",
      "  prompt_type  temperature   density\n",
      "0     complex         0.25  0.021817\n",
      "1     complex         0.50  0.020250\n",
      "2     complex         1.25  0.018980\n",
      "3     complex         1.50  0.018121\n",
      "4     complex         0.75  0.020179\n"
     ]
    }
   ],
   "source": [
    "# Initialize analyzer\n",
    "network_analyzer = NetworkAnalyzer()\n",
    "\n",
    "# Analyze networks\n",
    "results_file = os.path.join(DIRS['results'], 'network_metrics.csv')\n",
    "if not os.path.exists(results_file):\n",
    "    all_results = []\n",
    "    for prompt_type in PROMPTS.keys():\n",
    "        edge_dir = f'emo_edges_{prompt_type}'\n",
    "        if os.path.exists(edge_dir):\n",
    "            for edge_file in glob.glob(os.path.join(edge_dir, '*.txt')):\n",
    "                result = network_analyzer.analyze_edge_list(edge_file)\n",
    "                if result:\n",
    "                    all_results.append(result)\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(results_file, index=False)\n",
    "    print(f\"✓ Analyzed {len(df)} networks\")\n",
    "else:\n",
    "    df = pd.read_csv(results_file)\n",
    "    print(f\"✓ Loaded {len(df)} network metrics from cache\")\n",
    "\n",
    "print(f\"  Metrics: {len(df.columns) - 3} per network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2dafe",
   "metadata": {},
   "source": [
    "## 5. Semantic Analysis (BERTScore)\n",
    "\n",
    "Advanced semantic similarity analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5ee7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADVANCED SEMANTIC ANALYSIS WITH BERTSCORE\n",
      "Loading texts for semantic analysis...\n",
      "Loaded 14 texts for semantic analysis\n",
      "Computing BERTScore similarities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed 91 BERTScore similarity pairs\n",
      "BERTScore results saved to semantic_analysis/bertscore_similarities.csv\n",
      "Analyzing semantic coherence patterns...\n",
      "\n",
      "ENHANCED SEMANTIC COHERENCE ANALYSIS:\n",
      "Same prompt type coherence:\n",
      "                  mean       std  count    median       min       max\n",
      "text1_prompt                                                         \n",
      "complex       0.864552  0.011658     21  0.861355  0.846952  0.892121\n",
      "vague         0.866313  0.010609     21  0.869610  0.847549  0.880628\n",
      "\n",
      "Cross prompt type coherence:\n",
      "  Mean F1: 0.8253 ± 0.0053\n",
      "  Median F1: 0.8253\n",
      "  Range: [0.8129, 0.8377]\n",
      "\n",
      "Temperature-based coherence (same temperature pairs):\n",
      "   temperature   mean_f1  std_f1  count\n",
      "0        0.001  0.820467     NaN      1\n",
      "1        0.250  0.825259     NaN      1\n",
      "2        0.500  0.831417     NaN      1\n",
      "3        0.750  0.827367     NaN      1\n",
      "4        1.000  0.827270     NaN      1\n",
      "5        1.250  0.828069     NaN      1\n",
      "6        1.500  0.826234     NaN      1\n",
      "\n",
      "Temperature correlation analysis:\n",
      "  pearson: r = 0.0047, p = 0.9645 n.s.\n",
      "  spearman: r = -0.0014, p = 0.9893 n.s.\n",
      "  kendall: r = 0.0000, p = 1.0000 n.s.\n",
      "Performing topic modeling analysis...\n",
      "\n",
      "ENHANCED TOPIC MODELING RESULTS:\n",
      "Identified 10 topics\n",
      "Topic distribution by prompt type and temperature:\n",
      "  prompt_type  temperature  dominant_topic  count\n",
      "0     complex        0.001               5      1\n",
      "1     complex        0.250               5      1\n",
      "2     complex        0.500               5      1\n",
      "3     complex        0.750               5      1\n",
      "4     complex        1.000               5      1\n",
      "5     complex        1.250               5      1\n",
      "6     complex        1.500               5      1\n",
      "7       vague        0.001               7      1\n",
      "8       vague        0.250               7      1\n",
      "9       vague        0.500               7      1\n",
      "\n",
      "Topic diversity (entropy) by conditions:\n",
      "   prompt_type  temperature      mean  std\n",
      "0      complex        0.001  0.449429  NaN\n",
      "1      complex        0.250  0.450593  NaN\n",
      "2      complex        0.500  0.465836  NaN\n",
      "3      complex        0.750  0.483305  NaN\n",
      "4      complex        1.000  0.483986  NaN\n",
      "5      complex        1.250  0.496720  NaN\n",
      "6      complex        1.500  0.520158  NaN\n",
      "7        vague        0.001  0.357730  NaN\n",
      "8        vague        0.250  0.328827  NaN\n",
      "9        vague        0.500  0.319306  NaN\n",
      "10       vague        0.750  0.316328  NaN\n",
      "11       vague        1.000  0.309393  NaN\n",
      "12       vague        1.250  0.985112  NaN\n",
      "13       vague        1.500  0.310532  NaN\n",
      "Analyzing networks at multiple similarity thresholds...\n",
      "Error calculating enhanced metrics for threshold 0.25: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.25: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.3: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.3: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.35: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.35: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.4: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.4: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.45: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.45: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.5: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.5: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.6: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.6: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.7: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.7: 91 edges, density 1.000\n",
      "\n",
      "MULTI-THRESHOLD NETWORK ANALYSIS:\n",
      "   threshold  n_edges  density  n_components  avg_clustering\n",
      "0       0.25       91      1.0             1             1.0\n",
      "1       0.30       91      1.0             1             1.0\n",
      "2       0.35       91      1.0             1             1.0\n",
      "3       0.40       91      1.0             1             1.0\n",
      "4       0.45       91      1.0             1             1.0\n",
      "5       0.50       91      1.0             1             1.0\n",
      "6       0.60       91      1.0             1             1.0\n",
      "7       0.70       91      1.0             1             1.0\n",
      "\n",
      "Optimal threshold for analysis: 0.7\n",
      "OPTIMAL SEMANTIC SIMILARITY NETWORK (threshold=0.7):\n",
      "  n_nodes: 14\n",
      "  n_edges: 91\n",
      "  density: 1.0\n",
      "  avg_clustering: 1.0\n",
      "  n_components: 1\n",
      "  largest_component_size: 14\n",
      "  avg_path_length_largest_cc: 1.0\n",
      "Creating enhanced visualizations...\n",
      "\n",
      "ENHANCED TOPIC MODELING RESULTS:\n",
      "Identified 10 topics\n",
      "Topic distribution by prompt type and temperature:\n",
      "  prompt_type  temperature  dominant_topic  count\n",
      "0     complex        0.001               5      1\n",
      "1     complex        0.250               5      1\n",
      "2     complex        0.500               5      1\n",
      "3     complex        0.750               5      1\n",
      "4     complex        1.000               5      1\n",
      "5     complex        1.250               5      1\n",
      "6     complex        1.500               5      1\n",
      "7       vague        0.001               7      1\n",
      "8       vague        0.250               7      1\n",
      "9       vague        0.500               7      1\n",
      "\n",
      "Topic diversity (entropy) by conditions:\n",
      "   prompt_type  temperature      mean  std\n",
      "0      complex        0.001  0.449429  NaN\n",
      "1      complex        0.250  0.450593  NaN\n",
      "2      complex        0.500  0.465836  NaN\n",
      "3      complex        0.750  0.483305  NaN\n",
      "4      complex        1.000  0.483986  NaN\n",
      "5      complex        1.250  0.496720  NaN\n",
      "6      complex        1.500  0.520158  NaN\n",
      "7        vague        0.001  0.357730  NaN\n",
      "8        vague        0.250  0.328827  NaN\n",
      "9        vague        0.500  0.319306  NaN\n",
      "10       vague        0.750  0.316328  NaN\n",
      "11       vague        1.000  0.309393  NaN\n",
      "12       vague        1.250  0.985112  NaN\n",
      "13       vague        1.500  0.310532  NaN\n",
      "Analyzing networks at multiple similarity thresholds...\n",
      "Error calculating enhanced metrics for threshold 0.25: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.25: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.3: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.3: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.35: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.35: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.4: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.4: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.45: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.45: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.5: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.5: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.6: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.6: 91 edges, density 1.000\n",
      "Error calculating enhanced metrics for threshold 0.7: module 'community' has no attribute 'best_partition'\n",
      "Threshold 0.7: 91 edges, density 1.000\n",
      "\n",
      "MULTI-THRESHOLD NETWORK ANALYSIS:\n",
      "   threshold  n_edges  density  n_components  avg_clustering\n",
      "0       0.25       91      1.0             1             1.0\n",
      "1       0.30       91      1.0             1             1.0\n",
      "2       0.35       91      1.0             1             1.0\n",
      "3       0.40       91      1.0             1             1.0\n",
      "4       0.45       91      1.0             1             1.0\n",
      "5       0.50       91      1.0             1             1.0\n",
      "6       0.60       91      1.0             1             1.0\n",
      "7       0.70       91      1.0             1             1.0\n",
      "\n",
      "Optimal threshold for analysis: 0.7\n",
      "OPTIMAL SEMANTIC SIMILARITY NETWORK (threshold=0.7):\n",
      "  n_nodes: 14\n",
      "  n_edges: 91\n",
      "  density: 1.0\n",
      "  avg_clustering: 1.0\n",
      "  n_components: 1\n",
      "  largest_component_size: 14\n",
      "  avg_path_length_largest_cc: 1.0\n",
      "Creating enhanced visualizations...\n",
      "Enhanced visualizations saved to: semantic_analysis/comprehensive_semantic_analysis.png\n",
      "\n",
      "ENHANCED SEMANTIC ANALYSIS COMPLETED SUCCESSFULLY!\n",
      "Results saved in: semantic_analysis\n",
      "Key findings:\n",
      "  - Complex prompts show -0.2% higher coherence\n",
      "  - Temperature effects: ['pearson', 'spearman', 'kendall'] correlations computed\n",
      "  - Network analysis: 8 thresholds analyzed\n",
      "  - Topic modeling: 10 topics identified with diversity analysis\n",
      "Enhanced visualizations saved to: semantic_analysis/comprehensive_semantic_analysis.png\n",
      "\n",
      "ENHANCED SEMANTIC ANALYSIS COMPLETED SUCCESSFULLY!\n",
      "Results saved in: semantic_analysis\n",
      "Key findings:\n",
      "  - Complex prompts show -0.2% higher coherence\n",
      "  - Temperature effects: ['pearson', 'spearman', 'kendall'] correlations computed\n",
      "  - Network analysis: 8 thresholds analyzed\n",
      "  - Topic modeling: 10 topics identified with diversity analysis\n"
     ]
    }
   ],
   "source": [
    "# Enhanced semantic analysis with BERTScore\n",
    "print(\"ADVANCED SEMANTIC ANALYSIS WITH BERTSCORE\")\n",
    "\n",
    "# Configure enhanced analysis parameters\n",
    "ANALYSIS_CONFIG.update({\n",
    "    'similarity_thresholds': [0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.60, 0.70],  # More granular thresholds\n",
    "    'temperature_analysis': {\n",
    "        'min_pairs_per_temp': 1,  # Minimum pairs needed per temperature\n",
    "        'temperature_bins': 5,    # Number of bins for temperature difference analysis\n",
    "        'correlation_methods': ['pearson', 'spearman', 'kendall']\n",
    "    },\n",
    "    'n_topics': 10,\n",
    "    'alpha': 0.05\n",
    "})\n",
    "\n",
    "def load_texts_for_semantic_analysis():\n",
    "    \"\"\"Load all texts for semantic analysis\"\"\"\n",
    "    texts = []\n",
    "    metadata = []\n",
    "    \n",
    "    # Load all text files from both prompt types\n",
    "    for prompt_type in ['complex', 'vague']:\n",
    "        for temp in TEMPERATURES:\n",
    "            file_path = os.path.join(DIRS['texts'], f'mistral_{prompt_type}', f'{prompt_type}_{temp}.txt')\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read().strip()\n",
    "                    texts.append(text)\n",
    "                    metadata.append({\n",
    "                        'prompt_type': prompt_type,\n",
    "                        'temperature': temp,\n",
    "                        'filename': f'{prompt_type}_{temp}.txt',\n",
    "                        'text_idx': len(texts) - 1\n",
    "                    })\n",
    "    \n",
    "    return texts, metadata\n",
    "\n",
    "def compute_bertscore_similarities(texts, metadata):\n",
    "    \"\"\"Compute BERTScore similarities between all pairs of texts\"\"\"\n",
    "    print(\"Computing BERTScore similarities...\")\n",
    "    \n",
    "    # Initialize BERTScore\n",
    "    from bert_score import score\n",
    "    \n",
    "    bertscore_results = []\n",
    "    \n",
    "    # Compute pairwise similarities\n",
    "    for i in range(len(texts)):\n",
    "        for j in range(i + 1, len(texts)):\n",
    "            # Compute BERTScore\n",
    "            P, R, F1 = score([texts[i]], [texts[j]], lang='en', verbose=False)\n",
    "            \n",
    "            # Extract metadata\n",
    "            meta1, meta2 = metadata[i], metadata[j]\n",
    "            \n",
    "            # Create result record with enhanced features\n",
    "            result = {\n",
    "                'text1_idx': i,\n",
    "                'text2_idx': j,\n",
    "                'text1_prompt': meta1['prompt_type'],\n",
    "                'text2_prompt': meta2['prompt_type'],\n",
    "                'text1_temp': meta1['temperature'],\n",
    "                'text2_temp': meta2['temperature'],\n",
    "                'text1_filename': meta1['filename'],\n",
    "                'text2_filename': meta2['filename'],\n",
    "                'precision': float(P.item()),  # Convert to native Python float\n",
    "                'recall': float(R.item()),     # Convert to native Python float\n",
    "                'f1_score': float(F1.item()),  # Convert to native Python float\n",
    "                'same_prompt': meta1['prompt_type'] == meta2['prompt_type'],\n",
    "                'same_temp': meta1['temperature'] == meta2['temperature'],\n",
    "                'temp_diff': abs(meta1['temperature'] - meta2['temperature'])\n",
    "            }\n",
    "            \n",
    "            bertscore_results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(bertscore_results)\n",
    "\n",
    "def analyze_semantic_coherence_enhanced(bertscore_df):\n",
    "    \"\"\"Enhanced semantic coherence analysis with multiple perspectives\"\"\"\n",
    "    print(\"Analyzing semantic coherence patterns...\")\n",
    "    \n",
    "    coherence_results = {}\n",
    "    \n",
    "    # 1. Coherence within same prompt type\n",
    "    same_prompt_coherence = bertscore_df[\n",
    "        bertscore_df['text1_prompt'] == bertscore_df['text2_prompt']\n",
    "    ].groupby(['text1_prompt'])['f1_score'].agg(['mean', 'std', 'count', 'median', 'min', 'max'])\n",
    "    \n",
    "    coherence_results['same_prompt'] = same_prompt_coherence\n",
    "    \n",
    "    # 2. Coherence between different prompt types\n",
    "    cross_prompt_coherence = bertscore_df[\n",
    "        bertscore_df['text1_prompt'] != bertscore_df['text2_prompt']\n",
    "    ]['f1_score'].agg(['mean', 'std', 'count', 'median', 'min', 'max'])\n",
    "    \n",
    "    coherence_results['cross_prompt'] = cross_prompt_coherence\n",
    "    \n",
    "    # 3. Enhanced temperature-based coherence analysis\n",
    "    temp_coherence = []\n",
    "    \n",
    "    # Same temperature pairs\n",
    "    for temp in TEMPERATURES:\n",
    "        temp_data = bertscore_df[\n",
    "            (bertscore_df['text1_temp'] == temp) & \n",
    "            (bertscore_df['text2_temp'] == temp)\n",
    "        ]\n",
    "        if len(temp_data) >= ANALYSIS_CONFIG['temperature_analysis']['min_pairs_per_temp']:\n",
    "            temp_coherence.append({\n",
    "                'temperature': float(temp),  # Convert to native Python float\n",
    "                'analysis_type': 'same_temperature',\n",
    "                'mean_f1': float(temp_data['f1_score'].mean()),\n",
    "                'std_f1': float(temp_data['f1_score'].std()),\n",
    "                'median_f1': float(temp_data['f1_score'].median()),\n",
    "                'count': int(len(temp_data)),  # Convert to native Python int\n",
    "                'q25': float(temp_data['f1_score'].quantile(0.25)),\n",
    "                'q75': float(temp_data['f1_score'].quantile(0.75))\n",
    "            })\n",
    "    \n",
    "    # Temperature difference analysis\n",
    "    temp_diff_bins = pd.cut(bertscore_df['temp_diff'], \n",
    "                           bins=ANALYSIS_CONFIG['temperature_analysis']['temperature_bins'],\n",
    "                           include_lowest=True)\n",
    "    \n",
    "    temp_diff_analysis = bertscore_df.groupby(temp_diff_bins)['f1_score'].agg([\n",
    "        'mean', 'std', 'count', 'median'\n",
    "    ]).reset_index()\n",
    "    temp_diff_analysis['temp_diff_range'] = temp_diff_analysis['temp_diff'].astype(str)\n",
    "    \n",
    "    coherence_results['temperature'] = pd.DataFrame(temp_coherence)\n",
    "    coherence_results['temperature_difference'] = temp_diff_analysis\n",
    "    \n",
    "    # 4. Correlation analysis between temperature and semantic similarity\n",
    "    correlation_results = {}\n",
    "    for method in ANALYSIS_CONFIG['temperature_analysis']['correlation_methods']:\n",
    "        if method == 'pearson':\n",
    "            corr_coeff, p_value = stats.pearsonr(bertscore_df['temp_diff'], bertscore_df['f1_score'])\n",
    "        elif method == 'spearman':\n",
    "            corr_coeff, p_value = stats.spearmanr(bertscore_df['temp_diff'], bertscore_df['f1_score'])\n",
    "        elif method == 'kendall':\n",
    "            corr_coeff, p_value = stats.kendalltau(bertscore_df['temp_diff'], bertscore_df['f1_score'])\n",
    "        \n",
    "        correlation_results[method] = {\n",
    "            'correlation': float(corr_coeff),  # Convert to native Python float\n",
    "            'p_value': float(p_value),         # Convert to native Python float\n",
    "            'significant': bool(p_value < ANALYSIS_CONFIG['alpha'])  # Convert to native Python bool\n",
    "        }\n",
    "    \n",
    "    coherence_results['temperature_correlations'] = correlation_results\n",
    "    \n",
    "    return coherence_results\n",
    "\n",
    "def perform_topic_modeling(texts, metadata):\n",
    "    \"\"\"Perform enhanced topic modeling analysis\"\"\"\n",
    "    print(\"Performing topic modeling analysis...\")\n",
    "    \n",
    "    # Vectorize texts with enhanced parameters\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.8,\n",
    "        lowercase=True,\n",
    "        strip_accents='unicode'\n",
    "    )\n",
    "    \n",
    "    text_vectors = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Apply LDA with enhanced parameters\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=ANALYSIS_CONFIG['n_topics'],\n",
    "        random_state=42,\n",
    "        max_iter=100,\n",
    "        learning_method='batch',\n",
    "        evaluate_every=10,\n",
    "        perp_tol=0.1\n",
    "    )\n",
    "    \n",
    "    doc_topics = lda.fit_transform(text_vectors)\n",
    "    \n",
    "    # Create enhanced topic modeling results\n",
    "    topic_results = []\n",
    "    for i, doc_topic in enumerate(doc_topics):\n",
    "        dominant_topic = np.argmax(doc_topic)\n",
    "        topic_entropy = -np.sum(doc_topic * np.log(doc_topic + 1e-10))  # Topic entropy\n",
    "        \n",
    "        topic_results.append({\n",
    "            'text_idx': int(i),  # Convert to native Python int\n",
    "            'prompt_type': metadata[i]['prompt_type'],\n",
    "            'temperature': float(metadata[i]['temperature']),  # Convert to native Python float\n",
    "            'filename': metadata[i]['filename'],\n",
    "            'dominant_topic': int(dominant_topic),  # Convert to native Python int\n",
    "            'topic_probability': float(doc_topic[dominant_topic]),  # Convert to native Python float\n",
    "            'topic_entropy': float(topic_entropy),  # Convert to native Python float\n",
    "            'topic_distribution': [float(x) for x in doc_topic.tolist()]  # Convert all to native Python floats\n",
    "        })\n",
    "    \n",
    "    # Get enhanced topic words with weights\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic_words = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_word_indices = topic.argsort()[-10:][::-1]\n",
    "        top_words = [feature_names[i] for i in top_word_indices]\n",
    "        word_weights = topic[top_word_indices].tolist()\n",
    "        \n",
    "        topic_words.append({\n",
    "            'topic_id': int(topic_idx),  # Convert to native Python int\n",
    "            'top_words': top_words,\n",
    "            'word_weights': [float(w) for w in word_weights],  # Convert to native Python floats\n",
    "            'total_weight': float(np.sum(word_weights))  # Convert to native Python float\n",
    "        })\n",
    "    \n",
    "    # Topic distribution analysis by prompt and temperature\n",
    "    topic_df = pd.DataFrame(topic_results)\n",
    "    topic_distribution = topic_df.groupby(['prompt_type', 'temperature', 'dominant_topic']).size().reset_index(name='count')\n",
    "    topic_diversity = topic_df.groupby(['prompt_type', 'temperature'])['topic_entropy'].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    return topic_df, topic_words, topic_distribution, topic_diversity\n",
    "\n",
    "def analyze_network_at_multiple_thresholds(bertscore_df, thresholds):\n",
    "    \"\"\"Analyze semantic similarity networks at multiple thresholds\"\"\"\n",
    "    print(\"Analyzing networks at multiple similarity thresholds...\")\n",
    "    \n",
    "    threshold_results = []\n",
    "    detailed_networks = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        network, metrics = create_semantic_similarity_network(bertscore_df, threshold)\n",
    "        \n",
    "        # Enhanced metrics calculation with type conversion\n",
    "        enhanced_metrics = {}\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, (np.integer, np.floating)):\n",
    "                enhanced_metrics[key] = float(value) if isinstance(value, np.floating) else int(value)\n",
    "            else:\n",
    "                enhanced_metrics[key] = value\n",
    "        \n",
    "        if metrics['n_edges'] > 0:\n",
    "            # Calculate additional network properties\n",
    "            try:\n",
    "                degrees = [d for n, d in network.degree()]\n",
    "                enhanced_metrics['avg_degree'] = float(np.mean(degrees))\n",
    "                enhanced_metrics['max_degree'] = int(max(degrees))\n",
    "                \n",
    "                # Small world properties\n",
    "                if nx.is_connected(network):\n",
    "                    enhanced_metrics['diameter'] = int(nx.diameter(network))\n",
    "                    enhanced_metrics['radius'] = int(nx.radius(network))\n",
    "                    enhanced_metrics['average_shortest_path'] = float(nx.average_shortest_path_length(network))\n",
    "                \n",
    "                # Community detection (if available)\n",
    "                try:\n",
    "                    import community as community_louvain\n",
    "                    partition = community_louvain.best_partition(network)\n",
    "                    enhanced_metrics['n_communities'] = int(len(set(partition.values())))\n",
    "                    enhanced_metrics['modularity'] = float(community_louvain.modularity(partition, network))\n",
    "                except ImportError:\n",
    "                    enhanced_metrics['n_communities'] = None\n",
    "                    enhanced_metrics['modularity'] = None\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating enhanced metrics for threshold {threshold}: {e}\")\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'threshold': float(threshold),  # Convert to native Python float\n",
    "            **enhanced_metrics\n",
    "        })\n",
    "        \n",
    "        detailed_networks[threshold] = network\n",
    "        \n",
    "        print(f\"Threshold {threshold}: {metrics['n_edges']} edges, density {metrics['density']:.3f}\")\n",
    "    \n",
    "    return pd.DataFrame(threshold_results), detailed_networks\n",
    "\n",
    "def create_semantic_similarity_network(bertscore_df, threshold=0.7):\n",
    "    \"\"\"Create enhanced semantic similarity network based on BERTScore\"\"\"\n",
    "    \n",
    "    # Create network\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes (text indices) with metadata\n",
    "    unique_texts = set(bertscore_df['text1_idx'].unique()) | set(bertscore_df['text2_idx'].unique())\n",
    "    G.add_nodes_from(unique_texts)\n",
    "    \n",
    "    # Add edges based on similarity threshold with enhanced attributes\n",
    "    for _, row in bertscore_df.iterrows():\n",
    "        if row['f1_score'] >= threshold:\n",
    "            G.add_edge(\n",
    "                row['text1_idx'], \n",
    "                row['text2_idx'], \n",
    "                weight=float(row['f1_score']),\n",
    "                precision=float(row['precision']),\n",
    "                recall=float(row['recall']),\n",
    "                temp_diff=float(row['temp_diff']),\n",
    "                same_prompt=bool(row['same_prompt']),\n",
    "                same_temp=bool(row['same_temp'])\n",
    "            )\n",
    "    \n",
    "    # Compute enhanced network metrics\n",
    "    network_metrics = {\n",
    "        'n_nodes': int(G.number_of_nodes()),\n",
    "        'n_edges': int(G.number_of_edges()),\n",
    "        'density': float(nx.density(G) if G.number_of_nodes() > 1 else 0),\n",
    "        'avg_clustering': float(nx.average_clustering(G)),\n",
    "        'n_components': int(nx.number_connected_components(G))\n",
    "    }\n",
    "    \n",
    "    if G.number_of_edges() > 0:\n",
    "        try:\n",
    "            largest_cc = max(nx.connected_components(G), key=len)\n",
    "            largest_cc_subgraph = G.subgraph(largest_cc)\n",
    "            \n",
    "            if len(largest_cc) > 1:\n",
    "                network_metrics['largest_component_size'] = int(len(largest_cc))\n",
    "                network_metrics['avg_path_length_largest_cc'] = float(nx.average_shortest_path_length(largest_cc_subgraph))\n",
    "            else:\n",
    "                network_metrics['largest_component_size'] = 1\n",
    "                network_metrics['avg_path_length_largest_cc'] = 0.0\n",
    "        except:\n",
    "            network_metrics['largest_component_size'] = 0\n",
    "            network_metrics['avg_path_length_largest_cc'] = None\n",
    "    else:\n",
    "        network_metrics['largest_component_size'] = 0\n",
    "        network_metrics['avg_path_length_largest_cc'] = None\n",
    "    \n",
    "    return G, network_metrics\n",
    "\n",
    "def convert_for_json(obj):\n",
    "    \"\"\"Convert numpy types to native Python types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, pd.Series):\n",
    "        return obj.to_dict()\n",
    "    elif isinstance(obj, pd.DataFrame):\n",
    "        return {k: convert_for_json(v) for k, v in obj.to_dict().items()}\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_for_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_for_json(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def create_enhanced_visualizations(bertscore_df, coherence_results, threshold_results):\n",
    "    \"\"\"Create comprehensive visualizations for semantic analysis\"\"\"\n",
    "    print(\"Creating enhanced visualizations...\")\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('default')  # Use default style instead of seaborn\n",
    "    fig = plt.figure(figsize=(20, 24))\n",
    "    \n",
    "    # 1. BERTScore distribution by prompt type\n",
    "    ax1 = plt.subplot(4, 3, 1)\n",
    "    sns.boxplot(data=bertscore_df, x='text1_prompt', y='f1_score', ax=ax1)\n",
    "    ax1.set_title('BERTScore F1 Distribution by Prompt Type')\n",
    "    ax1.set_xlabel('Prompt Type')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    \n",
    "    # 2. Temperature difference vs semantic similarity\n",
    "    ax2 = plt.subplot(4, 3, 2)\n",
    "    scatter = ax2.scatter(bertscore_df['temp_diff'], bertscore_df['f1_score'], \n",
    "                         c=bertscore_df['same_prompt'].astype(int), \n",
    "                         alpha=0.6, cmap='viridis')\n",
    "    ax2.set_xlabel('Temperature Difference')\n",
    "    ax2.set_ylabel('F1 Score')\n",
    "    ax2.set_title('Temperature Difference vs Semantic Similarity')\n",
    "    plt.colorbar(scatter, ax=ax2, label='Same Prompt Type')\n",
    "    \n",
    "    # 3. Network metrics across thresholds\n",
    "    ax3 = plt.subplot(4, 3, 3)\n",
    "    ax3.plot(threshold_results['threshold'], threshold_results['n_edges'], 'o-', label='Edges')\n",
    "    ax3.set_xlabel('Similarity Threshold')\n",
    "    ax3.set_ylabel('Number of Edges')\n",
    "    ax3.set_title('Network Edges vs Threshold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Network density across thresholds\n",
    "    ax4 = plt.subplot(4, 3, 4)\n",
    "    ax4.plot(threshold_results['threshold'], threshold_results['density'], 's-', color='red', label='Density')\n",
    "    ax4.set_xlabel('Similarity Threshold')\n",
    "    ax4.set_ylabel('Network Density')\n",
    "    ax4.set_title('Network Density vs Threshold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Same vs cross-prompt coherence comparison\n",
    "    ax5 = plt.subplot(4, 3, 5)\n",
    "    if 'same_prompt' in coherence_results and not coherence_results['same_prompt'].empty:\n",
    "        prompt_means = coherence_results['same_prompt']['mean']\n",
    "        ax5.bar(range(len(prompt_means)), prompt_means.values, \n",
    "               yerr=coherence_results['same_prompt']['std'].values,\n",
    "               alpha=0.7, capsize=5)\n",
    "        ax5.set_xticks(range(len(prompt_means)))\n",
    "        ax5.set_xticklabels(prompt_means.index)\n",
    "        ax5.set_ylabel('Mean F1 Score')\n",
    "        ax5.set_title('Within-Prompt Type Coherence')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Temperature correlation analysis\n",
    "    ax6 = plt.subplot(4, 3, 6)\n",
    "    if 'temperature_correlations' in coherence_results:\n",
    "        methods = list(coherence_results['temperature_correlations'].keys())\n",
    "        correlations = [coherence_results['temperature_correlations'][m]['correlation'] for m in methods]\n",
    "        p_values = [coherence_results['temperature_correlations'][m]['p_value'] for m in methods]\n",
    "        \n",
    "        bars = ax6.bar(methods, correlations, alpha=0.7)\n",
    "        # Color bars based on significance\n",
    "        for i, (bar, p_val) in enumerate(zip(bars, p_values)):\n",
    "            if p_val < 0.05:\n",
    "                bar.set_color('red')\n",
    "            else:\n",
    "                bar.set_color('gray')\n",
    "        \n",
    "        ax6.set_ylabel('Correlation Coefficient')\n",
    "        ax6.set_title('Temperature-Similarity Correlations')\n",
    "        ax6.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. BERTScore component analysis\n",
    "    ax7 = plt.subplot(4, 3, 7)\n",
    "    ax7.scatter(bertscore_df['precision'], bertscore_df['recall'], \n",
    "               c=bertscore_df['f1_score'], alpha=0.6, cmap='plasma')\n",
    "    ax7.set_xlabel('Precision')\n",
    "    ax7.set_ylabel('Recall')\n",
    "    ax7.set_title('BERTScore Components Analysis')\n",
    "    \n",
    "    # 8. Network components analysis\n",
    "    ax8 = plt.subplot(4, 3, 8)\n",
    "    ax8.plot(threshold_results['threshold'], threshold_results['n_components'], 'd-', color='green')\n",
    "    ax8.set_xlabel('Similarity Threshold')\n",
    "    ax8.set_ylabel('Number of Components')\n",
    "    ax8.set_title('Network Fragmentation vs Threshold')\n",
    "    ax8.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. Temperature effect heatmap\n",
    "    ax9 = plt.subplot(4, 3, 9)\n",
    "    if 'temperature_difference' in coherence_results and not coherence_results['temperature_difference'].empty:\n",
    "        temp_data = coherence_results['temperature_difference']\n",
    "        if len(temp_data) > 1:\n",
    "            ax9.bar(range(len(temp_data)), temp_data['mean'], \n",
    "                   yerr=temp_data['std'], alpha=0.7, capsize=3)\n",
    "            ax9.set_xticks(range(len(temp_data)))\n",
    "            ax9.set_xticklabels([str(x) for x in temp_data['temp_diff_range']], rotation=45)\n",
    "            ax9.set_ylabel('Mean F1 Score')\n",
    "            ax9.set_title('Similarity by Temperature Difference')\n",
    "            ax9.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 10. Distribution of F1 scores\n",
    "    ax10 = plt.subplot(4, 3, 10)\n",
    "    ax10.hist(bertscore_df['f1_score'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax10.axvline(bertscore_df['f1_score'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    ax10.axvline(bertscore_df['f1_score'].median(), color='green', linestyle='--', label='Median')\n",
    "    ax10.set_xlabel('F1 Score')\n",
    "    ax10.set_ylabel('Frequency')\n",
    "    ax10.set_title('F1 Score Distribution')\n",
    "    ax10.legend()\n",
    "    ax10.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 11. Clustering coefficient analysis\n",
    "    ax11 = plt.subplot(4, 3, 11)\n",
    "    if 'avg_clustering' in threshold_results.columns:\n",
    "        ax11.plot(threshold_results['threshold'], threshold_results['avg_clustering'], 'o-', color='purple')\n",
    "        ax11.set_xlabel('Similarity Threshold')\n",
    "        ax11.set_ylabel('Average Clustering Coefficient')\n",
    "        ax11.set_title('Network Clustering vs Threshold')\n",
    "        ax11.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 12. Summary statistics table\n",
    "    ax12 = plt.subplot(4, 3, 12)\n",
    "    ax12.axis('off')\n",
    "    \n",
    "    # Create summary text\n",
    "    summary_text = f\"\"\"\n",
    "    SEMANTIC ANALYSIS SUMMARY\n",
    "    \n",
    "    Total text pairs analyzed: {len(bertscore_df)}\n",
    "    Mean F1 Score: {bertscore_df['f1_score'].mean():.3f} ± {bertscore_df['f1_score'].std():.3f}\n",
    "    \n",
    "    Within-prompt coherence:\n",
    "    Complex: {coherence_results['same_prompt'].loc['complex', 'mean']:.3f} ± {coherence_results['same_prompt'].loc['complex', 'std']:.3f}\n",
    "    Vague: {coherence_results['same_prompt'].loc['vague', 'mean']:.3f} ± {coherence_results['same_prompt'].loc['vague', 'std']:.3f}\n",
    "    \n",
    "    Cross-prompt coherence: {coherence_results['cross_prompt']['mean']:.3f} ± {coherence_results['cross_prompt']['std']:.3f}\n",
    "    \n",
    "    Optimal threshold: {threshold_results[threshold_results['n_edges'] > 0]['threshold'].min():.1f}\n",
    "    Max network density: {threshold_results['density'].max():.3f}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax12.text(0.1, 0.9, summary_text, transform=ax12.transAxes, fontsize=10,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the comprehensive visualization\n",
    "    viz_path = os.path.join(semantic_results_dir, 'comprehensive_semantic_analysis.png')\n",
    "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Enhanced visualizations saved to: {viz_path}\")\n",
    "    return viz_path\n",
    "\n",
    "# Main enhanced semantic analysis execution\n",
    "print(\"Loading texts for semantic analysis...\")\n",
    "texts, metadata = load_texts_for_semantic_analysis()\n",
    "\n",
    "if texts:\n",
    "    print(f\"Loaded {len(texts)} texts for semantic analysis\")\n",
    "    \n",
    "    # Compute BERTScore similarities\n",
    "    bertscore_df = compute_bertscore_similarities(texts, metadata)\n",
    "    \n",
    "    if not bertscore_df.empty:\n",
    "        print(f\"Computed {len(bertscore_df)} BERTScore similarity pairs\")\n",
    "        \n",
    "        # Save BERTScore results\n",
    "        bertscore_file = os.path.join(semantic_results_dir, 'bertscore_similarities.csv')\n",
    "        bertscore_df.to_csv(bertscore_file, index=False)\n",
    "        print(f\"BERTScore results saved to {bertscore_file}\")\n",
    "        \n",
    "        # Enhanced semantic coherence analysis\n",
    "        coherence_results = analyze_semantic_coherence_enhanced(bertscore_df)\n",
    "        \n",
    "        print(\"\\nENHANCED SEMANTIC COHERENCE ANALYSIS:\")\n",
    "        print(\"Same prompt type coherence:\")\n",
    "        print(coherence_results['same_prompt'])\n",
    "        \n",
    "        print(f\"\\nCross prompt type coherence:\")\n",
    "        cross_prompt = coherence_results['cross_prompt']\n",
    "        print(f\"  Mean F1: {cross_prompt['mean']:.4f} ± {cross_prompt['std']:.4f}\")\n",
    "        print(f\"  Median F1: {cross_prompt['median']:.4f}\")\n",
    "        print(f\"  Range: [{cross_prompt['min']:.4f}, {cross_prompt['max']:.4f}]\")\n",
    "        \n",
    "        if not coherence_results['temperature'].empty:\n",
    "            print(\"\\nTemperature-based coherence (same temperature pairs):\")\n",
    "            print(coherence_results['temperature'][['temperature', 'mean_f1', 'std_f1', 'count']])\n",
    "        \n",
    "        print(\"\\nTemperature correlation analysis:\")\n",
    "        for method, result in coherence_results['temperature_correlations'].items():\n",
    "            significance = \"***\" if result['significant'] else \"n.s.\"\n",
    "            print(f\"  {method}: r = {result['correlation']:.4f}, p = {result['p_value']:.4f} {significance}\")\n",
    "        \n",
    "        # Enhanced topic modeling\n",
    "        topic_results, topic_words, topic_distribution, topic_diversity = perform_topic_modeling(texts, metadata)\n",
    "        \n",
    "        if not topic_results.empty:\n",
    "            print(f\"\\nENHANCED TOPIC MODELING RESULTS:\")\n",
    "            print(f\"Identified {ANALYSIS_CONFIG['n_topics']} topics\")\n",
    "            \n",
    "            # Save enhanced topic results\n",
    "            topic_file = os.path.join(semantic_results_dir, 'enhanced_topic_modeling_results.csv')\n",
    "            topic_results.to_csv(topic_file, index=False)\n",
    "            \n",
    "            topic_dist_file = os.path.join(semantic_results_dir, 'topic_distribution_analysis.csv')\n",
    "            topic_distribution.to_csv(topic_dist_file, index=False)\n",
    "            \n",
    "            topic_div_file = os.path.join(semantic_results_dir, 'topic_diversity_analysis.csv')\n",
    "            topic_diversity.to_csv(topic_div_file, index=False)\n",
    "            \n",
    "            # Display enhanced topic summary\n",
    "            print(\"Topic distribution by prompt type and temperature:\")\n",
    "            print(topic_distribution.head(10))\n",
    "            \n",
    "            print(\"\\nTopic diversity (entropy) by conditions:\")\n",
    "            print(topic_diversity)\n",
    "        \n",
    "        # Multi-threshold network analysis\n",
    "        threshold_results, detailed_networks = analyze_network_at_multiple_thresholds(\n",
    "            bertscore_df, \n",
    "            ANALYSIS_CONFIG['similarity_thresholds']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nMULTI-THRESHOLD NETWORK ANALYSIS:\")\n",
    "        print(threshold_results[['threshold', 'n_edges', 'density', 'n_components', 'avg_clustering']])\n",
    "        \n",
    "        # Find optimal threshold (first threshold with edges)\n",
    "        thresholds_with_edges = threshold_results[threshold_results['n_edges'] > 0]\n",
    "        if not thresholds_with_edges.empty:\n",
    "            optimal_threshold = thresholds_with_edges['threshold'].max()  # Use highest threshold with edges\n",
    "            print(f\"\\nOptimal threshold for analysis: {optimal_threshold}\")\n",
    "            \n",
    "            # Create network with optimal threshold\n",
    "            optimal_network, optimal_metrics = create_semantic_similarity_network(\n",
    "                bertscore_df, \n",
    "                threshold=optimal_threshold\n",
    "            )\n",
    "            \n",
    "            print(f\"OPTIMAL SEMANTIC SIMILARITY NETWORK (threshold={optimal_threshold}):\")\n",
    "            for metric, value in optimal_metrics.items():\n",
    "                print(f\"  {metric}: {value}\")\n",
    "        else:\n",
    "            print(f\"\\nNo threshold produced edges. Consider lowering thresholds further.\")\n",
    "            optimal_threshold = ANALYSIS_CONFIG['similarity_threshold']\n",
    "        \n",
    "        # Save network analysis results\n",
    "        threshold_file = os.path.join(semantic_results_dir, 'multi_threshold_analysis.csv')\n",
    "        threshold_results.to_csv(threshold_file, index=False)\n",
    "        \n",
    "        # Save network metrics with proper JSON serialization\n",
    "        network_file = os.path.join(semantic_results_dir, 'enhanced_semantic_network_metrics.json')\n",
    "        import json\n",
    "        \n",
    "        # Convert all data to JSON-serializable format\n",
    "        network_data = {\n",
    "            'analysis_parameters': convert_for_json(ANALYSIS_CONFIG),\n",
    "            'threshold_analysis': convert_for_json(threshold_results.to_dict('records')),\n",
    "            'coherence_results': {\n",
    "                'same_prompt': convert_for_json(coherence_results['same_prompt'].to_dict()),\n",
    "                'cross_prompt': convert_for_json(coherence_results['cross_prompt'].to_dict()),\n",
    "                'temperature_correlations': convert_for_json(coherence_results['temperature_correlations'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(network_file, 'w') as f:\n",
    "            json.dump(network_data, f, indent=2)\n",
    "        \n",
    "        # Create enhanced visualizations\n",
    "        viz_path = create_enhanced_visualizations(bertscore_df, coherence_results, threshold_results)\n",
    "        \n",
    "        print(f\"\\nENHANCED SEMANTIC ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"Results saved in: {semantic_results_dir}\")\n",
    "        print(f\"Key findings:\")\n",
    "        print(f\"  - Complex prompts show {(coherence_results['same_prompt'].loc['complex', 'mean'] / coherence_results['same_prompt'].loc['vague', 'mean'] - 1) * 100:.1f}% higher coherence\")\n",
    "        print(f\"  - Temperature effects: {list(coherence_results['temperature_correlations'].keys())} correlations computed\")\n",
    "        print(f\"  - Network analysis: {len(ANALYSIS_CONFIG['similarity_thresholds'])} thresholds analyzed\")\n",
    "        print(f\"  - Topic modeling: {ANALYSIS_CONFIG['n_topics']} topics identified with diversity analysis\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No BERTScore similarities computed\")\n",
    "else:\n",
    "    print(\"No texts found for semantic analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12821ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing statistical analyzer...\n",
      "Loaded network metrics: 14 networks\n",
      "Starting comprehensive statistical analysis...\n",
      "Starting comprehensive statistical analysis...\n",
      "Running normality tests...\n",
      "Running normality tests...\n",
      "Running pairwise comparisons...\n",
      "Running pairwise comparisons...\n",
      "Running ANOVA analysis...\n",
      "Running ANOVA analysis...\n",
      "Calculating effect sizes...\n",
      "Running effect size calculations...\n",
      "Running PCA analysis...\n",
      "Running PCA analysis...\n",
      "Running clustering analysis...\n",
      "Running clustering analysis...\n",
      "Running regression analysis...\n",
      "Running regression analysis...\n",
      "Advanced analysis completed: 84 results generated\n",
      "Statistical analysis completed successfully!\n",
      "Total results: 84\n",
      "Results saved to: results/advanced_statistical_analysis.csv\n",
      "Analysis summary:\n",
      "  - normality_test: 20 results\n",
      "  - correlation: 20 results\n",
      "  - group_comparison: 10 results\n",
      "  - anova: 10 results\n",
      "  - effect_size: 10 results\n",
      "  - pca: 10 results\n",
      "  - clustering: 4 results\n",
      "\n",
      "============================================================\n",
      "ENHANCED BASIC STATISTICS\n",
      "============================================================\n",
      "\n",
      "Descriptive Statistics by Prompt Type:\n",
      "            density                                \n",
      "              count    mean     std     min     max\n",
      "prompt_type                                        \n",
      "complex           7  0.0205  0.0021  0.0181  0.0245\n",
      "vague             7  0.0164  0.0010  0.0150  0.0174\n",
      "\n",
      "Temperature Effect Analysis:\n",
      "               min  max    mean     std\n",
      "prompt_type                            \n",
      "complex      0.001  1.5  0.7501  0.5398\n",
      "vague        0.001  1.5  0.7501  0.5398\n",
      "\n",
      "Correlation Matrix:\n",
      "             density  temperature\n",
      "density        1.000       -0.506\n",
      "temperature   -0.506        1.000\n",
      "\n",
      "Statistical analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize statistical analyzer\n",
    "print(\"Initializing statistical analyzer...\")\n",
    "stat_analyzer = StatisticalAnalyzer(alpha=ANALYSIS_CONFIG['alpha'])\n",
    "\n",
    "# Check if we have network metrics data\n",
    "results_file = os.path.join(DIRS['results'], 'network_metrics.csv')\n",
    "\n",
    "if not os.path.exists(results_file):\n",
    "    print(\"Network metrics file not found. Please run network analysis first.\")\n",
    "    df = pd.DataFrame()\n",
    "else:\n",
    "    # Load network metrics\n",
    "    df = pd.read_csv(results_file)\n",
    "    print(f\"Loaded network metrics: {len(df)} networks\")\n",
    "    \n",
    "    # Check if advanced analysis has already been performed\n",
    "    advanced_results_file = os.path.join(DIRS['results'], 'advanced_statistical_analysis.csv')\n",
    "    \n",
    "    if os.path.exists(advanced_results_file):\n",
    "        print(\"Advanced statistical analysis already completed!\")\n",
    "        advanced_df = pd.read_csv(advanced_results_file)\n",
    "        print(f\"Analysis results: {len(advanced_df)} records\")\n",
    "    else:\n",
    "        print(\"Starting comprehensive statistical analysis...\")\n",
    "        \n",
    "        # Run advanced analysis using the fixed StatisticalAnalyzer class\n",
    "        advanced_df = stat_analyzer.run_advanced_analysis(df)\n",
    "        \n",
    "        if not advanced_df.empty:\n",
    "            # Save results\n",
    "            advanced_df.to_csv(advanced_results_file, index=False)\n",
    "            print(f\"Statistical analysis completed successfully!\")\n",
    "            print(f\"Total results: {len(advanced_df)}\")\n",
    "            print(f\"Results saved to: {advanced_results_file}\")\n",
    "            \n",
    "            # Display summary of analysis types\n",
    "            analysis_summary = advanced_df['analysis_type'].value_counts()\n",
    "            print(f\"Analysis summary:\")\n",
    "            for analysis_type, count in analysis_summary.items():\n",
    "                print(f\"  - {analysis_type}: {count} results\")\n",
    "        else:\n",
    "            print(\"No analysis results generated.\")\n",
    "    \n",
    "    # Display enhanced basic statistics\n",
    "    if not df.empty:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ENHANCED BASIC STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        key_metrics = ['num_nodes', 'num_edges', 'density', 'clustering_coefficient']\n",
    "        available_metrics = [col for col in key_metrics if col in df.columns]\n",
    "        \n",
    "        if available_metrics:\n",
    "            print(\"\\nDescriptive Statistics by Prompt Type:\")\n",
    "            summary_stats = df.groupby('prompt_type')[available_metrics].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "            print(summary_stats.round(4))\n",
    "            \n",
    "            print(\"\\nTemperature Effect Analysis:\")\n",
    "            temp_stats = df.groupby('prompt_type')['temperature'].agg(['min', 'max', 'mean', 'std'])\n",
    "            print(temp_stats.round(4))\n",
    "            \n",
    "            # Clean correlation matrix\n",
    "            corr_data = df[available_metrics + ['temperature']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            if not corr_data.empty:\n",
    "                print(\"\\nCorrelation Matrix:\")\n",
    "                corr_matrix = corr_data.corr()\n",
    "                print(corr_matrix.round(3))\n",
    "            \n",
    "        print(f\"\\nStatistical analysis completed successfully!\")\n",
    "    else:\n",
    "        print(\"No data available for statistical analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c661e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOTSTRAP ANALYSIS FOR ROBUST STATISTICAL INFERENCE\n",
      "Starting comprehensive bootstrap analysis...\n",
      "Performing bootstrap analysis with 1000 samples...\n",
      "Bootstrap confidence intervals computed for 20 metric-prompt combinations\n",
      "Bootstrap results saved to bootstrap_results/network_metrics_bootstrap.csv\n",
      "\n",
      "BOOTSTRAP CONFIDENCE INTERVALS (95%):\n",
      "  complex - nodes: 1347.2301 [1192.1700, 1482.6850]\n",
      "  complex - edges: 18547.9497 [15623.3714, 21091.8607]\n",
      "  complex - density: 0.0205 [0.0193, 0.0221]\n",
      "  complex - clustering: 0.5037 [0.5017, 0.5056]\n",
      "  complex - path_len: 2.5416 [2.5343, 2.5505]\n",
      "  complex - avg_deg: 27.2637 [25.8367, 28.4866]\n",
      "  complex - max_deg: 456.7406 [401.6611, 503.7214]\n",
      "  complex - std_deg: 41.2590 [38.0863, 43.9759]\n",
      "  complex - diameter: 5.2862 [5.0000, 5.6667]\n",
      "  complex - transitivity: 0.1985 [0.1947, 0.2026]\n",
      "Analyzing temperature effects with bootstrap...\n",
      "\n",
      "TEMPERATURE EFFECT CONFIDENCE INTERVALS:\n",
      "  nodes: r = 0.7779 [0.5587, 0.9131]\n",
      "  edges: r = 0.8737 [0.7479, 0.9540]\n",
      "  density: r = -0.4842 [-0.8432, 0.0548]\n",
      "  clustering: r = -0.0161 [-0.5924, 0.5751]\n",
      "  path_len: r = 0.0471 [-0.6121, 0.6622]\n",
      "  avg_deg: r = 0.8783 [0.7963, 0.9562]\n",
      "  max_deg: r = 0.5175 [0.0542, 0.8167]\n",
      "  std_deg: r = 0.9144 [0.8524, 0.9762]\n",
      "  diameter: r = -0.0374 [nan, nan]\n",
      "  transitivity: r = -0.1437 [-0.6898, 0.5017]\n",
      "Computing effect sizes...\n",
      "\n",
      "EFFECT SIZES:\n",
      "  nodes: Cohen's d = -1.3113 (large)\n",
      "  edges: Cohen's d = -0.7840 (medium)\n",
      "  density: Cohen's d = 2.5357 (large)\n",
      "  clustering: Cohen's d = 8.7549 (large)\n",
      "  path_len: Cohen's d = -3.3478 (large)\n",
      "  avg_deg: Cohen's d = 0.2823 (small)\n",
      "  max_deg: Cohen's d = -2.4215 (large)\n",
      "  std_deg: Cohen's d = 0.1736 (negligible)\n",
      "  diameter: Cohen's d = -1.0894 (large)\n",
      "  transitivity: Cohen's d = 6.1108 (large)\n",
      "Bootstrap analysis of semantic coherence...\n",
      "\n",
      "SEMANTIC COHERENCE BOOTSTRAP:\n",
      "                               mean       std\n",
      "analysis_type prompt_type                    \n",
      "cross_prompt  all          0.825326  0.000774\n",
      "within_prompt complex      0.864785  0.002576\n",
      "              vague        0.866209  0.002328\n",
      "\n",
      "Bootstrap analysis completed!\n",
      "Results saved in: bootstrap_results\n"
     ]
    }
   ],
   "source": [
    "## 7.2. Bootstrap Analysis for Robust Statistical Inference\n",
    "\n",
    "# This section implements bootstrap resampling to provide robust confidence intervals and statistical validation for all network metrics and semantic analysis results.\n",
    "\n",
    "# Bootstrap Analysis Implementation\n",
    "print(\"BOOTSTRAP ANALYSIS FOR ROBUST STATISTICAL INFERENCE\")\n",
    "\n",
    "def bootstrap_network_metrics(df, n_bootstrap=1000, ci_level=0.95):\n",
    "    \"\"\"\n",
    "    Perform bootstrap resampling on network metrics\n",
    "    \"\"\"\n",
    "    print(f\"Performing bootstrap analysis with {n_bootstrap} samples...\")\n",
    "    \n",
    "    bootstrap_results = []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Remove non-metric columns\n",
    "    metric_cols = [col for col in numeric_cols if col not in ['temperature']]\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        bootstrap_sample = df.sample(n=len(df), replace=True, random_state=i)\n",
    "        \n",
    "        # Compute metrics for each group\n",
    "        for prompt_type in df['prompt_type'].unique():\n",
    "            prompt_data = bootstrap_sample[bootstrap_sample['prompt_type'] == prompt_type]\n",
    "            \n",
    "            for col in metric_cols:\n",
    "                if col in prompt_data.columns:\n",
    "                    bootstrap_results.append({\n",
    "                        'bootstrap_id': i,\n",
    "                        'prompt_type': prompt_type,\n",
    "                        'metric': col,\n",
    "                        'value': prompt_data[col].mean(),\n",
    "                        'std': prompt_data[col].std(),\n",
    "                        'median': prompt_data[col].median()\n",
    "                    })\n",
    "    \n",
    "    bootstrap_df = pd.DataFrame(bootstrap_results)\n",
    "    \n",
    "    # Compute confidence intervals\n",
    "    alpha = 1 - ci_level\n",
    "    lower_percentile = (alpha/2) * 100\n",
    "    upper_percentile = (1 - alpha/2) * 100\n",
    "    \n",
    "    ci_results = []\n",
    "    for prompt_type in df['prompt_type'].unique():\n",
    "        for metric in metric_cols:\n",
    "            metric_data = bootstrap_df[\n",
    "                (bootstrap_df['prompt_type'] == prompt_type) & \n",
    "                (bootstrap_df['metric'] == metric)\n",
    "            ]['value']\n",
    "            \n",
    "            if len(metric_data) > 0:\n",
    "                ci_results.append({\n",
    "                    'prompt_type': prompt_type,\n",
    "                    'metric': metric,\n",
    "                    'mean': metric_data.mean(),\n",
    "                    'std': metric_data.std(),\n",
    "                    'ci_lower': np.percentile(metric_data, lower_percentile),\n",
    "                    'ci_upper': np.percentile(metric_data, upper_percentile),\n",
    "                    'original_mean': df[df['prompt_type'] == prompt_type][metric].mean()\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(ci_results), bootstrap_df\n",
    "\n",
    "def bootstrap_temperature_effects(df, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Bootstrap analysis of temperature effects on network metrics\n",
    "    \"\"\"\n",
    "    print(\"Analyzing temperature effects with bootstrap...\")\n",
    "    \n",
    "    bootstrap_temp_results = []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    metric_cols = [col for col in numeric_cols if col not in ['temperature']]\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        bootstrap_sample = df.sample(n=len(df), replace=True, random_state=i)\n",
    "        \n",
    "        for metric in metric_cols:\n",
    "            if metric in bootstrap_sample.columns:\n",
    "                # Compute correlation with temperature\n",
    "                temp_corr = bootstrap_sample['temperature'].corr(bootstrap_sample[metric])\n",
    "                \n",
    "                bootstrap_temp_results.append({\n",
    "                    'bootstrap_id': i,\n",
    "                    'metric': metric,\n",
    "                    'temperature_correlation': temp_corr\n",
    "                })\n",
    "    \n",
    "    temp_bootstrap_df = pd.DataFrame(bootstrap_temp_results)\n",
    "    \n",
    "    # Compute confidence intervals for correlations\n",
    "    temp_ci_results = []\n",
    "    for metric in metric_cols:\n",
    "        metric_corrs = temp_bootstrap_df[temp_bootstrap_df['metric'] == metric]['temperature_correlation']\n",
    "        \n",
    "        if len(metric_corrs) > 0:\n",
    "            temp_ci_results.append({\n",
    "                'metric': metric,\n",
    "                'mean_correlation': metric_corrs.mean(),\n",
    "                'std_correlation': metric_corrs.std(),\n",
    "                'ci_lower': np.percentile(metric_corrs, 2.5),\n",
    "                'ci_upper': np.percentile(metric_corrs, 97.5),\n",
    "                'original_correlation': df['temperature'].corr(df[metric])\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(temp_ci_results)\n",
    "\n",
    "def bootstrap_semantic_coherence(bertscore_df, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Bootstrap analysis of semantic coherence metrics\n",
    "    \"\"\"\n",
    "    print(\"Bootstrap analysis of semantic coherence...\")\n",
    "    \n",
    "    bootstrap_coherence_results = []\n",
    "    \n",
    "    for i in range(n_bootstrap):\n",
    "        bootstrap_sample = bertscore_df.sample(n=len(bertscore_df), replace=True, random_state=i)\n",
    "        \n",
    "        # Within-prompt coherence\n",
    "        same_prompt_data = bootstrap_sample[\n",
    "            bootstrap_sample['text1_prompt'] == bootstrap_sample['text2_prompt']\n",
    "        ]\n",
    "        \n",
    "        if len(same_prompt_data) > 0:\n",
    "            for prompt_type in same_prompt_data['text1_prompt'].unique():\n",
    "                prompt_data = same_prompt_data[same_prompt_data['text1_prompt'] == prompt_type]\n",
    "                \n",
    "                bootstrap_coherence_results.append({\n",
    "                    'bootstrap_id': i,\n",
    "                    'analysis_type': 'within_prompt',\n",
    "                    'prompt_type': prompt_type,\n",
    "                    'mean_f1': prompt_data['f1_score'].mean(),\n",
    "                    'std_f1': prompt_data['f1_score'].std()\n",
    "                })\n",
    "        \n",
    "        # Cross-prompt coherence\n",
    "        cross_prompt_data = bootstrap_sample[\n",
    "            bootstrap_sample['text1_prompt'] != bootstrap_sample['text2_prompt']\n",
    "        ]\n",
    "        \n",
    "        if len(cross_prompt_data) > 0:\n",
    "            bootstrap_coherence_results.append({\n",
    "                'bootstrap_id': i,\n",
    "                'analysis_type': 'cross_prompt',\n",
    "                'prompt_type': 'all',\n",
    "                'mean_f1': cross_prompt_data['f1_score'].mean(),\n",
    "                'std_f1': cross_prompt_data['f1_score'].std()\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(bootstrap_coherence_results)\n",
    "\n",
    "def compute_effect_sizes(df):\n",
    "    \"\"\"\n",
    "    Compute effect sizes for differences between prompt types\n",
    "    \"\"\"\n",
    "    print(\"Computing effect sizes...\")\n",
    "    \n",
    "    effect_sizes = []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    metric_cols = [col for col in numeric_cols if col not in ['temperature']]\n",
    "    \n",
    "    if len(df['prompt_type'].unique()) >= 2:\n",
    "        prompt_types = df['prompt_type'].unique()\n",
    "        \n",
    "        for metric in metric_cols:\n",
    "            if metric in df.columns:\n",
    "                group1_data = df[df['prompt_type'] == prompt_types[0]][metric].dropna()\n",
    "                group2_data = df[df['prompt_type'] == prompt_types[1]][metric].dropna()\n",
    "                \n",
    "                if len(group1_data) > 0 and len(group2_data) > 0:\n",
    "                    # Cohen's d\n",
    "                    pooled_std = np.sqrt(((len(group1_data) - 1) * group1_data.var() + \n",
    "                                        (len(group2_data) - 1) * group2_data.var()) / \n",
    "                                       (len(group1_data) + len(group2_data) - 2))\n",
    "                    \n",
    "                    cohens_d = (group1_data.mean() - group2_data.mean()) / pooled_std\n",
    "                    \n",
    "                    # Hedges' g (bias-corrected)\n",
    "                    correction_factor = 1 - (3 / (4 * (len(group1_data) + len(group2_data) - 2) - 1))\n",
    "                    hedges_g = cohens_d * correction_factor\n",
    "                    \n",
    "                    effect_sizes.append({\n",
    "                        'metric': metric,\n",
    "                        'group1': prompt_types[0],\n",
    "                        'group2': prompt_types[1],\n",
    "                        'group1_mean': group1_data.mean(),\n",
    "                        'group2_mean': group2_data.mean(),\n",
    "                        'cohens_d': cohens_d,\n",
    "                        'hedges_g': hedges_g,\n",
    "                        'interpretation': interpret_effect_size(abs(cohens_d))\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(effect_sizes)\n",
    "\n",
    "def interpret_effect_size(effect_size):\n",
    "    \"\"\"\n",
    "    Interpret effect size magnitude\n",
    "    \"\"\"\n",
    "    if effect_size < 0.2:\n",
    "        return \"negligible\"\n",
    "    elif effect_size < 0.5:\n",
    "        return \"small\"\n",
    "    elif effect_size < 0.8:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"large\"\n",
    "\n",
    "# Execute bootstrap analysis\n",
    "if 'df' in locals() and not df.empty:\n",
    "    print(\"Starting comprehensive bootstrap analysis...\")\n",
    "    \n",
    "    # Network metrics bootstrap\n",
    "    ci_results, bootstrap_df = bootstrap_network_metrics(\n",
    "        df, \n",
    "        n_bootstrap=ANALYSIS_CONFIG['n_bootstrap'],\n",
    "        ci_level=ANALYSIS_CONFIG['bootstrap_ci']\n",
    "    )\n",
    "    \n",
    "    if not ci_results.empty:\n",
    "        print(f\"Bootstrap confidence intervals computed for {len(ci_results)} metric-prompt combinations\")\n",
    "        \n",
    "        # Save bootstrap results\n",
    "        bootstrap_file = os.path.join(DIRS['bootstrap'], 'network_metrics_bootstrap.csv')\n",
    "        ci_results.to_csv(bootstrap_file, index=False)\n",
    "        print(f\"Bootstrap results saved to {bootstrap_file}\")\n",
    "        \n",
    "        # Display key results\n",
    "        print(\"\\nBOOTSTRAP CONFIDENCE INTERVALS (95%):\")\n",
    "        for _, row in ci_results.head(10).iterrows():\n",
    "            print(f\"  {row['prompt_type']} - {row['metric']}: \"\n",
    "                  f\"{row['mean']:.4f} [{row['ci_lower']:.4f}, {row['ci_upper']:.4f}]\")\n",
    "    \n",
    "    # Temperature effects bootstrap\n",
    "    temp_ci_results = bootstrap_temperature_effects(df, n_bootstrap=ANALYSIS_CONFIG['n_bootstrap'])\n",
    "    \n",
    "    if not temp_ci_results.empty:\n",
    "        print(f\"\\nTEMPERATURE EFFECT CONFIDENCE INTERVALS:\")\n",
    "        for _, row in temp_ci_results.head(10).iterrows():\n",
    "            print(f\"  {row['metric']}: r = {row['mean_correlation']:.4f} \"\n",
    "                  f\"[{row['ci_lower']:.4f}, {row['ci_upper']:.4f}]\")\n",
    "    \n",
    "    # Effect sizes\n",
    "    effect_sizes = compute_effect_sizes(df)\n",
    "    \n",
    "    if not effect_sizes.empty:\n",
    "        print(f\"\\nEFFECT SIZES:\")\n",
    "        for _, row in effect_sizes.head(10).iterrows():\n",
    "            print(f\"  {row['metric']}: Cohen's d = {row['cohens_d']:.4f} ({row['interpretation']})\")\n",
    "        \n",
    "        # Save effect sizes\n",
    "        effect_file = os.path.join(DIRS['bootstrap'], 'effect_sizes.csv')\n",
    "        effect_sizes.to_csv(effect_file, index=False)\n",
    "    \n",
    "    # Semantic coherence bootstrap (if available)\n",
    "    if 'bertscore_df' in locals() and not bertscore_df.empty:\n",
    "        semantic_bootstrap = bootstrap_semantic_coherence(bertscore_df, n_bootstrap=500)  # Reduced for efficiency\n",
    "        \n",
    "        if not semantic_bootstrap.empty:\n",
    "            print(f\"\\nSEMANTIC COHERENCE BOOTSTRAP:\")\n",
    "            semantic_summary = semantic_bootstrap.groupby(['analysis_type', 'prompt_type'])['mean_f1'].agg(['mean', 'std'])\n",
    "            print(semantic_summary)\n",
    "            \n",
    "            # Save semantic bootstrap results\n",
    "            semantic_bootstrap_file = os.path.join(DIRS['bootstrap'], 'semantic_coherence_bootstrap.csv')\n",
    "            semantic_bootstrap.to_csv(semantic_bootstrap_file, index=False)\n",
    "    \n",
    "    print(f\"\\nBootstrap analysis completed!\")\n",
    "    print(f\"Results saved in: {DIRS['bootstrap']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No network metrics data available for bootstrap analysis\")\n",
    "    print(\"Please run network analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7ca1db",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Create analysis visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1227ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network visualizer...\n",
      "Loaded network metrics for visualization: 14 networks\n",
      "Creating overview plots...\n",
      "Overview plots created: figures/network_overview.png\n",
      "Creating correlation analysis...\n",
      "Error creating correlation heatmap: 'NetworkVisualizer' object has no attribute 'create_correlation_heatmap'\n",
      "Creating comparative analysis...\n",
      "Error creating comparative analysis: 'NetworkVisualizer' object has no attribute 'create_comparative_analysis'\n",
      "\n",
      "Available visualizations:\n",
      "  - network_overview.png\n",
      "\n",
      "Visualization summary:\n",
      "  - Total networks visualized: 14\n",
      "  - Prompt types: complex, vague\n",
      "  - Temperature range: 0.001 - 1.5\n",
      "  - Figures created: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = NetworkVisualizer(\n",
    "    style=ANALYSIS_CONFIG['style'],\n",
    "    figsize=ANALYSIS_CONFIG['figsize']\n",
    ")\n",
    "\n",
    "# Create visualizations\n",
    "if os.path.exists(os.path.join(DIRS['results'], 'network_metrics.csv')):\n",
    "    df = pd.read_csv(os.path.join(DIRS['results'], 'network_metrics.csv'))\n",
    "    \n",
    "    visualizer.create_overview_plots(df, save_path=os.path.join(DIRS['figures'], 'network_overview.png'))\n",
    "    visualizer.create_correlation_heatmap(df, save_path=os.path.join(DIRS['figures'], 'correlation_heatmap.png'))\n",
    "    visualizer.create_comparative_analysis(df, save_path=os.path.join(DIRS['figures'], 'comparative_analysis.png'))\n",
    "    \n",
    "    viz_files = glob.glob(os.path.join(DIRS['figures'], '*.png'))\n",
    "    print(f\"✓ Created {len(viz_files)} visualizations\")\n",
    "else:\n",
    "    print(\"⚠ No network metrics found. Run analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80746506",
   "metadata": {},
   "source": [
    "## 7. Results Summary\n",
    "\n",
    "Generate comprehensive pipeline summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e536bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEMANTIC ANALYSIS PIPELINE - RESULTS SUMMARY\n",
      "\n",
      "TEXT GENERATION & PREPROCESSING:\n",
      "complex_original: 7 files\n",
      "vague_original: 7 files\n",
      "complex_cleaned: 7 files\n",
      "vague_cleaned: 7 files\n",
      "\n",
      "SEMANTIC NETWORKS:\n",
      "Complex networks: 7 files\n",
      "Vague networks: 7 files\n",
      "\n",
      "ANALYSIS RESULTS:\n",
      "network_metrics: 14 records\n",
      "advanced_analysis: Not found\n",
      "detailed_results: Not found\n",
      "\n",
      "VISUALIZATIONS:\n",
      "Total figures: 1\n",
      "    - network_overview.png\n",
      "\n",
      "PIPELINE STATUS:\n",
      "Completion rate: 100.0%\n",
      "Completed components: 4/4\n",
      "Pipeline completed successfully!\n",
      "\n",
      "Summary report saved to: results/pipeline_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Pipeline summary\n",
    "summary = {\n",
    "    'texts_generated': sum(len(glob.glob(os.path.join(DIRS['texts'], f\"{DIR_PREFIX}_{pt}\", '*.txt'))) \n",
    "                          for pt in PROMPTS.keys() if os.path.exists(os.path.join(DIRS['texts'], f\"{DIR_PREFIX}_{pt}\"))),\n",
    "    'texts_cleaned': sum(len(glob.glob(os.path.join(DIRS['texts'], f\"cleaned_{DIR_PREFIX}_{pt}\", '*.txt'))) \n",
    "                        for pt in PROMPTS.keys() if os.path.exists(os.path.join(DIRS['texts'], f\"cleaned_{DIR_PREFIX}_{pt}\"))),\n",
    "    'networks_built': sum(len(glob.glob(os.path.join(f'emo_edges_{pt}', '*.txt'))) \n",
    "                         for pt in PROMPTS.keys() if os.path.exists(f'emo_edges_{pt}')),\n",
    "    'visualizations': len(glob.glob(os.path.join(DIRS['figures'], '*.png')))\n",
    "}\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Save summary\n",
    "summary_file = os.path.join(DIRS['results'], 'pipeline_summary.json')\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"✓ Summary saved to {summary_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
